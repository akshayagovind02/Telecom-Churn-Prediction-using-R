---
title: "Machine Learning Project"
---

```{r warning = FALSE, message = FALSE}
# Suppress dplyr summaries grouping warning messages
options(dplyr.summarise.inform = FALSE)
## Add R libraries here
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(skimr)
library(vip)
library (ggpubr) 
# Load the dataset
telecom_df <- readRDS(url('https://gmubusinessanalytics.netlify.app/data/telecom_df.rds'))
write.csv(telecom_df,'telecom.csv')

```
# Data Analysis

```{r}

skim(telecom_df)

```

```{r}
# View data frame properties and summary statistics
skim(telecom_df, avg_call_mins, avg_intl_mins,months_with_company,monthly_charges)
```

# Question 1


**Question**:
How is the company performing? What percentage of the subscribers have cancelled the service

**Answer**:
From the below table/plot we can say 36% of the subscribers have cancelled the service. This is quiet high and should be investigated.  

```{r}

cancel_perct <- telecom_df %>% group_by(canceled_service) %>% summarize(count = n()) %>% mutate(percent = prop.table(count)*100)
cancel_perct

ggplot(cancel_perct, aes(x = canceled_service,y= percent, fill = canceled_service)) + geom_col() +
      labs(title = "Percentage Total Vs Cancelled Service",
           x = "Cancelled Service",
           y = "% of Cancelled Service")

```
# Question 2


**Question**:
Are the customers cancelling the service long term customers or recent ones?

**Answer**:
Most of the customers who have canceled the service has stayed within a smaller time period than the customers who has not cancelled the service. The monthly charges falls within the bracket of the retaining customers.

```{r}
telecom_df %>%select(canceled_service,monthly_charges,months_with_company) %>% group_by (canceled_service) %>%summarize(avg_monthly_charges = mean(monthly_charges),avg_months_with_company = mean(months_with_company))
telecom_df %>%select(canceled_service,monthly_charges,months_with_company) %>% group_by (canceled_service) %>%summary
a <- ggplot(telecom_df, aes(y= months_with_company, x = "", fill = canceled_service)) + geom_boxplot() +
      labs(title = "Month with Company Vs Cancelled Service",
           x = "Cancelled Service",
           y = "Months With Company")
b <- ggplot(telecom_df, aes(y= monthly_charges, x = "", fill = canceled_service)) + geom_boxplot() +
      labs(title = "Monthly Charges Vs Cancelled Service",
           x = "Cancelled Service",
           y = "Monthsly Charges")
figure <- ggarrange(a, b,ncol=2,nrow = 1,common.legend = T, align = c("hv"),legend="top")
figure
```


# Question 3

**Question**:

Does having a dependent in plan contribute to unsubscribing a service?

**Answer**:

As seen in the table those who don't have a spouse partner or dependents contribute more to cancelling the service than with dependency

```{r}

telecom_df %>%
  group_by(canceled_service) %>%
  summarize(#senior_citizen_yes = sum(senior_citizen == 'yes'),senior_citizen_no = sum(senior_citizen == 'no'),
            spouse_partner_yes = sum(spouse_partner == 'yes'),spouse_partner_no = sum(spouse_partner == 'no'),
            dependents_yes = sum(dependents == 'yes'),dependents_no = sum(dependents == 'no'))

#a <- ggplot(telecom_df, aes(x=senior_citizen,fill=canceled_service))+ geom_bar(position = 'fill')
b <- ggplot(telecom_df, aes(x=spouse_partner,fill=canceled_service))+ geom_bar(position = 'fill')
c <- ggplot(telecom_df, aes(x=dependents,fill=canceled_service))+ geom_bar(position = 'fill')
figure <- ggarrange(b, c,ncol=2,common.legend = T, align = c("hv"),legend="top")
figure
```
# Question 4


**Question**:

What are the different Cellular lines and Internet service subscription associated with the service.

**Answer**:

Cellular service line and Multiple line have almost equal customers who have cancelled the service, though single line cellular service has more customers in the canceled list.
Internet Service - 'Fiber Optic' has more customers who have cancelled the service than the internet service 'digital'. The difference is quiet significant and needs to be investigated

```{r}

telecom_df %>%
  group_by(canceled_service) %>%
  summarize(cellular_service_sl = sum(cellular_service == 'single_line'),cellular_service_ml = sum(cellular_service == 'multiple_lines'),
            internet_service_d = sum(internet_service == 'digital'),internet_service_fo = sum(internet_service == 'fiber_optic'))

d <- ggplot(telecom_df, aes(x=cellular_service,fill=canceled_service))+ geom_bar(position = 'fill')
e <- ggplot(telecom_df, aes(x=internet_service,fill=canceled_service))+ geom_bar(position = 'fill')

figure <- ggarrange(d,e,ncol=2,common.legend = T, align = c("hv"),legend="top")
figure
```

# Question 5


**Question**:

What is the effect of security and backup protection and support w.r.t cancelling the service

**Answer**:

The four attributes almost follow a similar pattern where customers not opting online security ,online backup, device protection or tech support are at the risk of cancelling the service.

```{r}

telecom_df %>%
  group_by(canceled_service) %>%
  summarize(online_security_yes = sum(online_security == 'yes'),online_security_no = sum(online_security == 'no'),
            online_backup_yes = sum(online_backup == 'yes'),online_backup_no = sum(online_backup == 'no'),
            device_protection_yes = sum(device_protection == 'yes'),device_protection_no = sum(device_protection == 'no'),
            tech_support_yes = sum(tech_support == 'yes'),tech_support_no = sum(tech_support == 'no'))

f <- ggplot(telecom_df, aes(x=online_security,fill=canceled_service))+ geom_bar(position = 'fill')
g <- ggplot(telecom_df, aes(x=online_backup,fill=canceled_service))+ geom_bar(position = 'fill')
h <- ggplot(telecom_df, aes(x=device_protection,fill=canceled_service))+ geom_bar(position = 'fill')
i <- ggplot(telecom_df, aes(x=tech_support,fill=canceled_service))+ geom_bar(position = 'fill')
figure <- ggarrange(f,g,h,i,ncol=2,nrow = 2,common.legend = T, align = c("hv"),legend="top")
figure
```
# Question 6


**Question**:

What is the effect of streaming tv and movies w.r.t cancelling the service

**Answer**:

There could be a problem with streaming services as customers subscribed to streaming service has higher percentage of cancenlation than customer who havent subscribed to streaming scervices

```{r}

telecom_df %>%
  group_by(canceled_service) %>%
  summarize(streaming_tv_yes = sum(streaming_tv == 'yes'),streaming_tv_no = sum(streaming_tv == 'no'),
            streaming_movies_yes = sum(streaming_movies == 'yes'),streaming_movies_no = sum(streaming_movies == 'no'))

f <- ggplot(telecom_df, aes(x=streaming_tv,fill=canceled_service))+ geom_bar(position = 'fill')
g <- ggplot(telecom_df, aes(x=streaming_movies,fill=canceled_service))+ geom_bar(position = 'fill')
figure <- ggarrange(f,g,ncol=2,nrow =1,common.legend = T, align = c("hv"),legend="top")
figure
```
# Question 7


**Question**:

Is signing a contract make an impact with customer retention ?

**Answer**:

Signing a contract for a longer duration makes a customer stay with a company. Those who have signed a month-to-month are at a risk of cancenlling the service as per history. 

```{r}

telecom_df %>%
  group_by(canceled_service) %>%
  summarize(streaming_tv_yes = sum(streaming_tv == 'yes'),streaming_tv_no = sum(streaming_tv == 'no'),
            streaming_movies_yes = sum(streaming_movies == 'yes'),streaming_movies_no = sum(streaming_movies == 'no'))

h <- ggplot(telecom_df, aes(x=contract,fill=canceled_service))+ geom_bar(position = 'fill')
i <- ggplot(telecom_df, aes(x=paperless_bill,fill=canceled_service))+ geom_bar(position = 'fill')

figure <- ggarrange(h,i,ncol=2,nrow =1,common.legend = T, align = c("hv"),legend="top")
figure
```
# Question 8


**Question**:

Which payment method has high attrition rate?

**Answer**:

Customers who have subscribed to electronic check have higher percentage of cancelling the service , followed by mailed check.
```{r}

ggplot(telecom_df, aes(x=payment_method,fill=canceled_service))+ geom_bar(position = 'fill')
```

# Question 9


**Question**:

What can be inferred from the call duration of the number and international calls 

**Answer**:

The call duration for the customers canceled and retaining their service are plotted in a histogram.For the customers,both the cancelled = Yes and No are centered around 300 to 400 avg call duration. And the international call duration is centered around 100.

```{r}
canceled_yes <- telecom_df %>% filter(canceled_service=='yes')
canceled_no <- telecom_df %>% filter(canceled_service=='no')  

a <- ggplot(canceled_yes,aes(x=avg_call_mins)) + geom_histogram(bins=20) + labs(title = "Distribution of Average Call Mins for 'Cancelled Service - 'Yes'")
b <- ggplot(canceled_yes,aes(x=avg_intl_mins)) + geom_histogram(bins=20) + labs(title = "Distribution of Average International Call Mins for 'Cancelled Service - 'Yes'")
c <- ggplot(canceled_no,aes(x=avg_call_mins)) + geom_histogram(bins=20) + labs(title = "Distribution of Average Call Mins for 'Cancelled Service - 'No'")
d <- ggplot(canceled_no,aes(x=avg_intl_mins)) + geom_histogram(bins=20) + labs(title = "Distribution of Average International Call Mins for 'Cancelled Service - 'No'")
figure <- ggarrange(a,c,ncol=1,nrow = 2,common.legend = T, align = c("hv"),legend="top")
figure
figure <- ggarrange(b,d,ncol=1,nrow = 2,common.legend = T, align = c("hv"),legend="top")
figure

```


# Question 10


**Question**:

Is there any visible Correlation between all the numeric variables

**Answer**:

Using the cor function and corr plot we can find the correlation between numerical variable.Monthly charges and Moths with a company has the maximum correlation i.e 0.468.If they are highly correlated , we can remove one of them as it may skew the results

```{r}
library(corrplot)
library(RColorBrewer)
M <-cor(telecom_df[, c('avg_call_mins','avg_intl_mins','months_with_company','monthly_charges')])
M
corrplot(M, type="upper", order="hclust",
         col=brewer.pal(n=8, name="RdYlBu"))


```
# Machine Learning


In this section of the project, we will fit **three classification algorithms** to predict the response variable,`canceled_service`. You should use all of the other variables in the `telecom_df` data as predictor variables for each model.

following the machine learning steps below. 

The data splitting and feature engineering steps should only be done once so that your models are using the same data and feature engineering steps for training.

- Split the `telecom_df` data into a training and test set (remember to set your seed)
- Specify a feature engineering pipeline with the `recipes` package
    - You can include steps such as skewness transformation, dummy variable encoding or any other steps you find appropriate
- Specify a `parsnip` model object
    - You may choose from the following classification algorithms:
      - Logistic Regression
      - LDA
      - QDA
      - KNN
      - Decision Tree
      - Random Forest
- Package your recipe and model into a workflow
- Fit your workflow to the training data
    - If your model has hyperparameters:
      - Split the training data into 5 folds for 5-fold cross validation using `vfold_cv` (remember to set your seed)
      - Perform hyperparamter tuning with a random grid search using the `grid_random()` function
      - Refer to the following tutorial for an example - [Random Grid Search](https://gmubusinessanalytics.netlify.app/lesson-08-r-tutorial.html#Hyperparameter_Tuning14){target="_blank"}
      - Hyperparameter tuning can take a significant amount of computing time. Be careful not to set the `size` argument of `grid_random()` too large. I recommend `size` = 10 or smaller.
      - Select the best model with `select_best()` and finalize your workflow
- Evaluate model performance on the test set by plotting an ROC curve using `autoplot()` and calculating the area under the ROC curve on your test data

```{r}

levels(telecom_df$canceled_service)
```

### Splitting our data with `initial_split()`.

```{r}

set.seed(314) 

telecom_split <- initial_split(telecom_df, prop = 0.75,strata = canceled_service)

telecom_training <- telecom_split %>% training()

telecom_test <- telecom_split %>% testing()

telecom_folds <- vfold_cv(telecom_training, v = 5)
```

### Feature Engineering Recipe

-   Remove skewness from numeric predictors
-   Normalize all numeric predictors
-   Create dummy variables for all nominal predictors

```{r}

telecom_recipe <- recipe(canceled_service ~ ., data = telecom_training) %>% 
                       step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
                       step_normalize(all_numeric(), -all_outcomes()) %>% 
                       step_dummy(all_nominal(), -all_outcomes())
```

Applying the recipe to the training dataset

```{r}

telecom_recipe %>% 
  prep(training = telecom_training) %>% 
  bake(new_data = NULL)
```
### Model 1 - KNN Model Specification

```{r}

knn_model <- nearest_neighbor(neighbors = tune()) %>% 
             set_engine('kknn') %>% 
             set_mode('classification')
```

### Creating a Workflow

As before the next step is to create a workflow with our recipe and model.

```{r}
knn_wf <- workflow() %>% 
          add_model(knn_model) %>% 
          add_recipe(telecom_recipe)
```

### Hyperparameter tuning


```{r}

k_grid <- tibble(neighbors = c(10, 20, 30, 50, 75, 100, 125, 150))
k_grid
```

`tune_grid()` function to determine the optimal value of our hyperparameter.

```{r}

## Tune  workflow
set.seed(314)
knn_tuning <- knn_wf %>% 
              tune_grid(resamples = telecom_folds,
                         grid = k_grid)
```


```{r}

## Show the top 5 best models based on roc_auc metric
knn_tuning %>% show_best('roc_auc')

```

`select_best()` model based on the `roc_auc` metric to select the model from our tuning results that had the best overall performance. 

```{r}

## Select best model based on roc_auc
best_k <- knn_tuning %>% 
          select_best(metric = 'roc_auc')

## View model
best_k
```

`finalize_workflow()` to add our optimal model to our workflow object.

```{r}

## Finalize workflow by adding the best performing model
final_knn_wf <- knn_wf %>% 
                finalize_workflow(best_k)
```

### Train and Evaluate With `last_fit()`

Train and estimate model performance on our test data set using the `last_fit()` function.

```{r}
last_fit_knn <- final_knn_wf %>% 
                last_fit(split = telecom_split)
```

The KNN - classification model has a accuracy of 0.76 and roc_auc as 0.85

```{r}

last_fit_knn %>% collect_metrics()
```

### ROC Curve

To get ROC curve and confusion matrix.

```{r}

knn_predictions <- last_fit_knn %>% 
                   collect_predictions()

knn_predictions
```

ROC plot by using `roc_curve()` and `autoplot()`.

```{r}

knn_predictions %>% 
  roc_curve(truth = canceled_service, estimate = .pred_yes) %>% 
  autoplot()
```

### Confusion Matrix

We see that our model made 45 false negatives and 25 false positives.

```{r}

conf_mat(knn_predictions, truth = canceled_service, estimate = .pred_class)
```

# Model 2 - #Decision Tree Model

##Model Specification

```{r}

tree_model <- decision_tree(cost_complexity = tune(),
                            tree_depth = tune(),
                            min_n = tune()) %>% 
              set_engine('rpart') %>% 
              set_mode('classification')
```

## Workflow

```{r}

tree_workflow <- workflow() %>% 
                 add_model(tree_model) %>% 
                 add_recipe(telecom_recipe)
```

## Hyperparameter Tuning

```{r}

tree_grid <- grid_regular(parameters(tree_model), 
                          levels = 2)
tree_grid
```

### Tuning Hyperparameters with `tune_grid()`

```{r}

## Tune decision tree workflow
set.seed(314)

tree_tuning <- tree_workflow %>% 
               tune_grid(resamples = telecom_folds,
                         grid = tree_grid)
```


```{r}
## Show the top 5 best models based on roc_auc metric
tree_tuning %>% show_best('roc_auc')
```

```{r}

## Select best model based on roc_auc
best_tree <- tree_tuning %>% 
             select_best(metric = 'roc_auc')

# View the best tree parameters
best_tree
```

### Finalize Workflow

```{r}

final_tree_workflow <- tree_workflow %>% 
                       finalize_workflow(best_tree)
```

### Fit the Model

Fit our workflow to the training data. 

```{r}

tree_wf_fit <- final_tree_workflow %>% 
               fit(data = telecom_training)
```

### Exploring our Trained Model

Extract the trained model from our workflow fit, `tree_wf_fit`. 

```{r warning = FALSE, message = FALSE}

tree_fit <- tree_wf_fit %>% 
            extract_fit_parsnip()
```

### Variable Importance

We see from the results below, that `months with company` by far is the most important predictor of canceled service

```{r}

vip(tree_fit)
```
## Train and Evaluate With `last_fit()`

Fit final model workflow to the training data and evaluate performance on the test data.

The `last_fit()` function will fit our workflow to the training data and generate predictions on the test data as defined by our `telecom_split` object.

```{r}

tree_last_fit <- final_tree_workflow %>% 
                 last_fit(telecom_split)
```
The Decision Tree model has a accuracy of 0.76 and roc_auc as 0.83
```{r}
tree_last_fit %>% collect_metrics()
```

### ROC Curve
```{r}

tree_last_fit %>% collect_predictions() %>% 
                  roc_curve(truth  = canceled_service, estimate = .pred_yes) %>% 
                  autoplot()
```

### Confusion Matrix

We see that our model made 34 false negatives and 35 false positives.

```{r}
tree_predictions <- tree_last_fit %>% collect_predictions()

conf_mat(tree_predictions, truth = canceled_service, estimate = .pred_class)
```

# Model 3 - Random Forest

```{r}

rf_model <- rand_forest(mtry = tune(),
                        trees = tune(),
                        min_n = tune()) %>% 
            set_engine('ranger', importance = "impurity") %>% 
            set_mode('classification')
```

## Workflow

```{r}

rf_workflow <- workflow() %>% 
               add_model(rf_model) %>% 
               add_recipe(telecom_recipe)
```

## Hyperparameter Tuning

### Random Grid Search

```{r}
set.seed(314)

rf_grid <- grid_random(mtry() %>% range_set(c(2, 4)),
                       trees(),
                       min_n(),
                       size = 10)
```

```{r}

rf_grid
```

### Tuning Hyperparameters with `tune_grid()`

Find the optimal combination of hyperparameters from tuning grid.

```{r}

## Tune random forest workflow
set.seed(314)

rf_tuning <- rf_workflow %>% 
             tune_grid(resamples = telecom_folds,
                       grid = rf_grid)
```

View the results of our hyperparameter tuning, we can use the `show_best()` function. We must pass the type of performance metric we would like to see into the `show_best()` function.

```{r}

## Show the top 5 best models based on roc_auc metric
rf_tuning %>% show_best('roc_auc')
```

We can use the `select_best()` model to select the model from our tuning results that had the best overall performance. In the code below, we specify to select the best performing model based on the `roc_auc` metric.

```{r}

## Select best model based on roc_auc
best_rf <- rf_tuning %>% 
           select_best(metric = 'roc_auc')

# View the best parameters
best_rf
```

### Finalize Workflow

Hyperparameter tuning to use `finalize_workflow()` to add our optimal model to our workflow object.

```{r}

final_rf_workflow <- rf_workflow %>% 
                     finalize_workflow(best_rf)
```

### Fit the Model

Fit our workflow to the training data by passing our workflow object to the `fit()` function.

```{r}

rf_wf_fit <- final_rf_workflow %>% 
             fit(data = telecom_training)
```

Once we have trained our model on our training data, we can study variable importance with the `vip()` function.

The first step is to extract the trained model from our workflow fit, `rf_wf_fit`. This can be done by passing `rf_wf_fit` to the `extract_fit_parnsip()` function.

```{r warning = FALSE, message = FALSE}

rf_fit <- rf_wf_fit %>% 
          extract_fit_parsnip()
```

### Variable Importance

We see from the results below, that `months_with_company` by far is the most important predictor of `canceled_service`.

```{r}

vip(rf_fit)
```

## Train and Evaluate With `last_fit()`

Next we fit our final model workflow to the training data and evaluate performance on the test data.

The `last_fit()` function will fit our workflow to the training data and generate predictions on the test data as defined by our `churn_split` object.

```{r}

rf_last_fit <- final_rf_workflow %>% 
               last_fit(telecom_split)
```

We can view our performance metrics on the test data.The Random Forest Classifier model has a accuracy of 0.70 and roc_auc as 0.87. This Model has so far the best results among the three models.

```{r}

rf_last_fit %>% collect_metrics()
```

### ROC Curve

ROC curve to visualize test set performance of our random forest model.

```{r}

rf_last_fit %>% collect_predictions() %>% 
                roc_curve(truth  = canceled_service, estimate = .pred_yes) %>% 
                autoplot()
```

### Confusion Matrix

We see that our model made 24 false negatives and 40 false positives on our test data set.

```{r}

rf_predictions <- rf_last_fit %>% collect_predictions()

conf_mat(rf_predictions, truth = canceled_service, estimate = .pred_class)
```


# Summary of Results

Write a summary of your overall findings and recommendations to the executives at the company. Think of this section as your closing remarks of a presentation, where you summarize your key findings, model performance, and make recommendations to improve customer retention and service at this company.

Your executive summary must be written in a [professional tone](https://www.universalclass.com/articles/writing/business-writing/appropriate-tone-in-business-communications.htm), with minimal grammatical errors, and should include the following sections:

1. An introduction where you explain the business problem and goals of your data analysis

    - What problem(s) is this company trying to solve? Why are they important to their future success?
  
    - What was the goal of your analysis? What questions were you trying to answer and why do they matter?


2. Highlights and key findings from your Exploratory Data Analysis section 
    - What were the interesting findings from your analysis and **why are they important for the business**?

    - This section is meant to **establish the need for your recommendations** in the following section


3. Your “best” classification model and an analysis of its performance 
    - In this section you should talk about the expected error of your model on future data
      - To estimate future performance, you can use your model performance results on the **test data**
    - You should discuss at least one performance metric, such as an F1, sensitivity, specificity, or ROC AUC for your model. However, you must explain the results in an **intuitive, non-technical manner**. Your audience in this case are executives at a telecommunications company with limited knowledge of machine learning.


4. Your recommendations to the company on how to reduce customer attrition rates 
  
    - Each recommendation must be supported by your data analysis results 

    - You must clearly explain why you are making each recommendation and which results from your data analysis support this recommendation

    - You must also describe the potential business impact of your recommendation:
      
      - Why is this a good recommendation? 
      
      - What benefits will the business achieve?

**Summary**

We are working on a telecom company dataset which has different attributes. The goal of this project is to analyse which attributes contribute to the customers cancelling a service.This analysis is import because it helps to identify the problem ares, bring some solutions to help with customer retention. The attrition rate is quiet high at 36.3% and we need to figure out how to bring this value down

From the exploratory analysis we can see the 
1. Customers who have dependents or spouses enrolled in their plan have low cancellation than those who does not have dependents.
2.We can see the customers cancelling the service have mostly stayed with the company for short duration.This can be confirmed by two attributes - months with the company and contract - on a month by month basis. 
3.The four attributes online security ,online backup, device protection or tech support almost follow a similar pattern where customers not opting online security ,online backup, device protection or tech support are at the risk of cancelling the service.
4.Customers who have opted for paperless payment have higher attrition . Specifically, Customers who have subscribed to electronic check have higher percentage of cancelling the service.

Among the three classification models used to predict whether a customer will cancel a service or not , the Random forest model gives the best result with an ROC_AUC of 87%. On plotting the variable importance, we can see that the 'months_with_company' attribute is the most important factor to decide if a customer will cancel a service. AUC measures how well a model is able to distinguish the different class. The AUC measure of 0.87 means the model has 87% chance to identify the classes correctly.


Customers who have dependents or spouses enrolled in their plan have low cancellation than those who does not have dependents. We can get some discounts  for customers who enroll in family plan there by attracting more customers and hence increasing the retention rate.We can see the customers cancelling the service have mostly stayed with the company for short duration.This can be confirmed by two attributes - months with the company and contract - on a month by month basis. We need some plans which encourage getting customers on a longer contract than a smaller one. The company can increase the monthly charges for the customers who enter a short contract considering the risk of leaving them and can have some kind of incentives like Pay for One and dependents get concession.The four attributes online security ,online backup, device protection or tech support almost follow a similar pattern where customers not opting online security ,online backup, device protection or tech support are at the risk of cancelling the service. We can give trial for these services so the customer can check and benefit from them.Customers who have opted for paperless payment have higher attrition . Specifically, Customers who have subscribed to electronic check have higher percentage of cancelling the service. The company can conduct a survey and get recommendations and feedback on the electronic payment service. By making changes to the contract and payment plan the company can see a significant in the retention rate     
